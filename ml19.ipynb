{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1a1fa672",
   "metadata": {},
   "source": [
    "1.A set of one-dimensional data points is given to you: 5, 10, 15, 20, 25, 30, 35. Assume that k = 2\n",
    "and that the first set of random centroid is 15, 32, and that the second set is 12, 30.\n",
    "a) Using the k-means method, create two clusters for each set of centroid described above.\n",
    "b) For each set of centroid values, calculate the SSE.\n",
    "\n",
    "ANS-\n",
    "a) Using the k-means method, we start by assigning each data point to its nearest centroid, then we update the centroids by calculating the mean of the points in each cluster. We repeat this process until convergence, i.e., when the assignment of points to clusters does not change.\n",
    "\n",
    "For the first set of centroids (15, 32), the initial assignment of points to clusters could be:\n",
    "\n",
    "Cluster 1: {5, 10, 15, 20, 25}\n",
    "Cluster 2: {30, 35}\n",
    "The mean of each cluster is:\n",
    "\n",
    "Cluster 1: (5+10+15+20+25)/5 = 15\n",
    "Cluster 2: (30+35)/2 = 32.5\n",
    "So the updated centroids are (15, 32.5). We then reassign points to clusters based on their distance to the new centroids:\n",
    "\n",
    "Cluster 1: {5, 10, 15, 20, 25}\n",
    "Cluster 2: {30, 35}\n",
    "The mean of each cluster is:\n",
    "\n",
    "Cluster 1: (5+10+15+20+25)/5 = 15\n",
    "Cluster 2: (30+35)/2 = 32.5\n",
    "The centroids did not change, so we have converged to a solution. The two clusters are {5, 10, 15, 20, 25} and {30, 35}.\n",
    "\n",
    "For the second set of centroids (12, 30), the initial assignment of points to clusters could be:\n",
    "\n",
    "Cluster 1: {5, 10, 15}\n",
    "Cluster 2: {20, 25, 30, 35}\n",
    "The mean of each cluster is:\n",
    "\n",
    "Cluster 1: (5+10+15)/3 = 10\n",
    "Cluster 2: (20+25+30+35)/4 = 27.5\n",
    "So the updated centroids are (10, 27.5). We then reassign points to clusters based on their distance to the new centroids:\n",
    "\n",
    "Cluster 1: {5, 10, 15}\n",
    "Cluster 2: {20, 25, 30, 35}\n",
    "The mean of each cluster is:\n",
    "\n",
    "Cluster 1: (5+10+15)/3 = 10\n",
    "Cluster 2: (20+25+30+35)/4 = 27.5\n",
    "The centroids did not change, so we have converged to a solution. The two clusters are {5, 10, 15} and {20, 25, 30, 35}.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "b) For each set of centroid values, we can calculate the SSE (Sum of Squared Errors) as the sum of the squared distances between each point and its centroid within the cluster it belongs to. The formula is:\n",
    "\n",
    "SSE = Σᵢ Σⱼ (xᵢⱼ - cⱼ)²\n",
    "\n",
    "where i is the index of the cluster, j is the index of the data point within the cluster, xᵢⱼ is the value of the j-th data point in the i-th cluster, and cⱼ is the value of the centroid for the i-th cluster.\n",
    "\n",
    "For the first set of centroids (15, 32), the SSE can be calculated as:\n",
    "\n",
    "SSE₁ = (5-15)² + (10-15)² + (15-15)² + (20-15)² + (25-15)² + (30-32)² + (35-32)²\n",
    "SSE₁ = 150 + 25 + 0 + 25"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78ba6c0f",
   "metadata": {},
   "source": [
    "2.Describe how the Market Basket Research makes use of association analysis concepts.\n",
    "\n",
    "ANS-\n",
    "Market Basket Analysis (MBA) is a data mining technique that uses Association Analysis concepts to discover relationships between items purchased by customers in a retail store or online. The goal of MBA is to find the co-occurrence of items in a transaction or a basket and generate rules that indicate the probability of an item being purchased with another item.\n",
    "\n",
    "Association Analysis is based on the Apriori Algorithm, which involves generating itemsets of different lengths and calculating their support, confidence, and lift measures. The support measures the frequency of occurrence of an item or an itemset in the data, while confidence measures the probability of an item being purchased given that another item is already in the basket. The lift measures the degree of dependence between two items and indicates the strength of association between them.\n",
    "\n",
    "Market Basket Research makes use of these association analysis concepts to identify the relationships between products that are most likely to be purchased together. Retailers can use this information to improve their marketing strategies, such as product placement, cross-selling, and targeted promotions. For example, if MBA reveals that customers who buy bread also tend to buy butter, the retailer could place these items close to each other on the shelves or offer a promotion on butter when a customer purchases bread.\n",
    "\n",
    "MBA can also help retailers to identify which items are commonly purchased together and which items are not. This can provide insights into the store's product offerings, inventory management, and supply chain operations. By analyzing customer purchase patterns, retailers can optimize their product mix and increase profitability by stocking more of the items that are frequently purchased together."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55a404b5",
   "metadata": {},
   "source": [
    "3.Give an example of the Apriori algorithm for learning association rules.\n",
    "\n",
    "ANS-\n",
    "Suppose we have a dataset of customer transactions in a grocery store, and we want to identify which items are frequently purchased together. Here is a sample of the data:\n",
    "\n",
    "Transaction #1: Milk, Bread, Cheese, Eggs\n",
    "Transaction #2: Milk, Bread, Cheese\n",
    "Transaction #3: Bread, Cheese, Butter\n",
    "Transaction #4: Milk, Bread, Eggs\n",
    "Transaction #5: Bread, Eggs\n",
    "\n",
    "The Apriori algorithm involves the following steps:\n",
    "\n",
    "1 -Determine the minimum support threshold\n",
    "The minimum support threshold is the minimum percentage of transactions in which an itemset must appear in order to be considered frequent. For example, if we set the minimum support threshold to 40%, then an itemset that appears in 2 out of 5 transactions (or 40% of the transactions) will be considered frequent.\n",
    "2 -Generate frequent itemsets of length 1\n",
    "In this step, we generate a list of all single-item sets (also known as 1-itemsets) that meet the minimum support threshold. For example, if the minimum support threshold is 40%, then the frequent 1-itemsets would be:\n",
    "Milk (appears in 3 out of 5 transactions)\n",
    "Bread (appears in 4 out of 5 transactions)\n",
    "Cheese (appears in 3 out of 5 transactions)\n",
    "Eggs (appears in 2 out of 5 transactions)\n",
    "Butter (appears in 1 out of 5 transactions)\n",
    "\n",
    "3 -Generate frequent itemsets of length 2\n",
    "In this step, we generate a list of all 2-itemsets that meet the minimum support threshold. We do this by joining pairs of frequent 1-itemsets together. For example, if the minimum support threshold is 40%, then the frequent 2-itemsets would be:\n",
    "{Milk, Bread} (appears in 3 out of 5 transactions)\n",
    "{Milk, Cheese} (appears in 2 out of 5 transactions)\n",
    "{Bread, Cheese} (appears in 3 out of 5 transactions)\n",
    "{Bread, Eggs} (appears in 2 out of 5 transactions)\n",
    "\n",
    "4 -Generate frequent itemsets of length 3\n",
    "In this step, we generate a list of all 3-itemsets that meet the minimum support threshold. We do this by joining pairs of frequent 2-itemsets together. For example, if the minimum support threshold is 40%, then the frequent 3-itemsets would be:\n",
    "{Milk, Bread, Cheese} (appears in 2 out of 5 transactions)\n",
    "\n",
    "5 -Generate association rules\n",
    "In this step, we generate all possible association rules from the frequent itemsets. An association rule is a statement of the form \"if X then Y\", where X and Y are itemsets. For example, if the minimum confidence threshold is 50%, then the following association rules would be generated:\n",
    "{Milk, Bread} -> {Cheese} (confidence = 2/3 = 67%)\n",
    "{Milk, Cheese} -> {Bread} (confidence = 2/2 = 100%)\n",
    "{Bread, Cheese} -> {Milk} (confidence = 2/3 = 67%)\n",
    "{Bread, Eggs} -> {Milk} (confidence = 2/2 = 100%)\n",
    "\n",
    "These association rules indicate that customers who purchase Milk and Bread are likely to purchase Cheese as well, and vice versa. Similarly, customers who purchase Bread and Eggs are likely to purchase Milk as well. Retailers can use this information to optimize their product placements and promotions, such as by offering a discount on Cheese when a customer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e015e4e",
   "metadata": {},
   "source": [
    "4.In hierarchical clustering, how is the distance between clusters measured? Explain how this metric\n",
    "is used to decide when to end the iteration.\n",
    "\n",
    "ANS-\n",
    "Some commonly used distance metrics include:\n",
    "\n",
    "1 -Euclidean distance: This is the straight-line distance between two points in Euclidean space.\n",
    "2 -Manhattan distance: This is the distance between two points measured along the axes of a rectangular coordinate system.\n",
    "\n",
    "To decide when to end the iteration, we use the concept of a dendrogram. A dendrogram is a tree-like diagram that shows the hierarchical relationship between the clusters. At the beginning of the iteration, each data point is considered a separate cluster. Then, at each iteration, the two closest clusters are merged into a single cluster, and this process is repeated until all data points belong to a single cluster."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0bbf5f7",
   "metadata": {},
   "source": [
    "5.In the k-means algorithm, how do you recompute the cluster centroids?\n",
    "\n",
    "ANS-\n",
    "In the k-means algorithm, the cluster centroids are recomputed in the following way:\n",
    "\n",
    "1 -Assign each data point to the cluster with the closest centroid, using a distance metric such as Euclidean distance.\n",
    "2 -For each cluster, compute the mean (or centroid) of the data points assigned to that cluster. This mean becomes the new centroid of that cluster.\n",
    "3 -Repeat steps 1 and 2 until convergence, i.e., until the cluster assignments and centroids no longer change or the change is below a certain threshold."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b45ee5c",
   "metadata": {},
   "source": [
    "6.At the start of the clustering exercise, discuss one method for determining the required number of\n",
    "clusters.\n",
    "\n",
    "ANS-\n",
    "1 -Elbow method: In this method, we plot the SSE (sum of squared errors) against the number of clusters, and look for the \"elbow point\" where the SSE starts to level off. This point represents the optimal number of clusters that minimizes the SSE while avoiding overfitting.\n",
    "\n",
    "2 -Hierarchical clustering: In this method, we use a dendrogram to visualize the hierarchical relationship between the clusters and choose the number of clusters that best fits the data based on the desired level of granularity.\n",
    "\n",
    "3 -Silhouette method: In this method, we compute the average silhouette coefficient for different values of k (number of clusters), where the silhouette coefficient measures the similarity of a data point to its own cluster compared to other clusters. The optimal number of clusters is the one that maximizes the average silhouette coefficient."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "874b17fe",
   "metadata": {},
   "source": [
    "7.Discuss the k-means algorithm's advantages and disadvantages.\n",
    "\n",
    "ANS-\n",
    "Advantages:\n",
    "1 -Fast and efficient: The k-means algorithm is relatively simple and fast, making it scalable to large datasets. It is also easily parallelizable and can be run on multiple processors to further speed up the computation.\n",
    "2 -Easy to implement: The algorithm is straightforward to implement and requires minimal computational resources.\n",
    "3 -Works well on data with well-defined clusters: K-means is particularly effective when the data points are well separated and there are distinct clusters.\n",
    "4 -Interpretable: The resulting clusters are easy to interpret, and the algorithm provides clear boundaries between the clusters.\n",
    "\n",
    "\n",
    "Disadvantages:\n",
    "1 -Sensitive to initialization: The k-means algorithm is sensitive to the initial choice of centroids and can result in different clustering results depending on the initial conditions. This requires running the algorithm multiple times with different initializations to obtain stable results.\n",
    "2 -Can converge to local optima: The algorithm can converge to local optima and may not find the global optimum, resulting in suboptimal clusters.\n",
    "3 -Assumes spherical clusters: The k-means algorithm assumes that the clusters are spherical and have similar sizes, which may not always be the case in real-world datasets.\n",
    "4 -Requires prior knowledge of the number of clusters: The algorithm requires the number of clusters to be specified in advance, which may not always be known a priori. This can lead to suboptimal clustering results if the wrong number of clusters is chosen.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85e636ea",
   "metadata": {},
   "source": [
    "8.Draw a diagram to demonstrate the principle of clustering.\n",
    "\n",
    "ANS-\n",
    "To demonstrate the principle of clustering, let's consider an example of clustering customer data for a retail store. The data set contains information about customers such as age, gender, income, and shopping habits. Our goal is to group similar customers together based on their attributes so that we can target our marketing efforts to specific customer groups.\n",
    "\n",
    "We can use the k-means clustering algorithm to partition the data set into clusters. The k-means algorithm works by iteratively updating cluster centroids until convergence is reached. Here's a simple example of the k-means algorithm in action:\n",
    "\n",
    "1 -Choose the number of clusters (k) to create. Let's say we want to create three clusters.\n",
    "2 -Select k random data points as the initial cluster centroids.\n",
    "3 -Assign each data point to the nearest centroid.\n",
    "4 -Compute the mean of each cluster to get the new centroid for each cluster.\n",
    "5 -Repeat steps 3-4 until convergence is reached (i.e., the cluster centroids no longer change).\n",
    "\n",
    "After running the k-means algorithm, we will end up with three clusters of customers, each containing customers with similar attributes. We can then analyze each cluster to understand the characteristics of the customers in that cluster and tailor our marketing efforts accordingly."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a9477a4",
   "metadata": {},
   "source": [
    "9.During your study, you discovered seven findings, which are listed in the data points below. Using\n",
    "the K-means algorithm, you want to build three clusters from these observations. The clusters C1,\n",
    "C2, and C3 have the following findings after the first iteration:\n",
    "\n",
    "C1: (2,2), (4,4), (6,6); C2: (2,2), (4,4), (6,6); C3: (2,2), (4,4),\n",
    "\n",
    "C2: (0,4), (4,0), (0,4), (0,4), (0,4), (0,4), (0,4), (0,4), (0,\n",
    "\n",
    "C3: (5,5) and (9,9)\n",
    "\n",
    "What would the cluster centroids be if you were to run a second iteration? What would this\n",
    "clustering's SSE be?\n",
    "\n",
    "ANS-\n",
    "C1 centroid: ((2+4+6)/3, (2+4+6)/3) = (4, 4)\n",
    "C2 centroid: ((0+4+0+0+0+0+0+0+0)/9, (4+0+4+4+4+4+4+4+4)/9) = (0.44, 3.56)\n",
    "C3 centroid: ((5+9)/2, (5+9)/2) = (7, 7)\n",
    "\n",
    "To compute the SSE, we would sum the squared distance between each data point and its assigned cluster centroid:\n",
    "\n",
    "SSE = ||(2,2) - (4,4)||^2 + ||(4,4) - (4,4)||^2 + ||(6,6) - (4,4)||^2\n",
    "+ ||(0,4) - (0.44, 3.56)||^2 + ||(4,0) - (0.44, 3.56)||^2\n",
    "+ ||(0,4) - (0.44, 3.56)||^2 + ||(0,4) - (0.44, 3.56)||^2\n",
    "+ ||(0,4) - (0.44, 3.56)||^2 + ||(0,4) - (0.44, 3.56)||^2\n",
    "+ ||(0,4) - (0.44, 3.56)||^2 + ||(0,4) - (0.44, 3.56)||^2\n",
    "+ ||(0,4) - (0.44, 3.56)||^2 + ||(0,4) - (0.44, 3.56)||^2\n",
    "+ ||(0,4) - (0.44, 3.56)||^2 + ||(5,5) - (7,7)||^2 + ||(9,9) - (7,7)||^2\n",
    "\n",
    "SSE = 74.56\n",
    "\n",
    "It is important to note that the result may be different if the data points or the initial centroids are changed, as the k-means algorithm is sensitive to the initialization and the order of data points."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "424d4f7e",
   "metadata": {},
   "source": [
    "10.In a software project, the team is attempting to determine if software flaws discovered during\n",
    "testing are identical. Based on the text analytics of the defect details, they decided to build 5 clusters\n",
    "of related defects. Any new defect formed after the 5 clusters of defects have been identified must\n",
    "be listed as one of the forms identified by clustering. A simple diagram can be used to explain this\n",
    "process. Assume you have 20 defect data points that are clustered into 5 clusters and you used the\n",
    "k-means algorithm.\n",
    "\n",
    "ANS-\n",
    "\n",
    "simple diagram that explains the clustering process for the software project:\n",
    "\n",
    "1 -Start with 20 defect data points.\n",
    "2 -Use the k-means algorithm to cluster the data points into 5 clusters.\n",
    "3 -Assign each data point to its closest cluster centroid.\n",
    "4 -Any new defect discovered after the clustering must be assigned to one of the 5 identified clusters.\n",
    "5 -If a new defect is sufficiently different from all 5 clusters, it may be necessary to create a new cluster.\n",
    "6 -The process can be repeated periodically to ensure that new defects are properly categorized and assigned to the appropriate cluster.\n",
    "\n",
    "Here's an example of what the clusters might look like after the k-means algorithm has been applied:\n",
    "\n",
    "Cluster 1: Defects 1, 3, 5, 7, 8\n",
    "Cluster 2: Defects 2, 4, 6, 10\n",
    "Cluster 3: Defects 9, 11, 12, 13\n",
    "Cluster 4: Defects 14, 15, 16, 17\n",
    "Cluster 5: Defects 18, 19, 20\n",
    "\n",
    "Once the clusters have been identified, any new defects that are discovered can be compared to the existing clusters to see if they are similar enough to be assigned to one of the existing clusters. If a new defect is sufficiently different from all of the existing clusters, a new cluster may need to be created to accommodate it."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "330aae31",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
