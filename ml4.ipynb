{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4bb1da26",
   "metadata": {},
   "source": [
    "1.What are the key tasks involved in getting ready to work with machine learning modeling?\n",
    "\n",
    "ANS-\n",
    "1 - Define the problem: Before starting any machine learning project, you need to have a clear understanding of the problem you're trying to solve. This involves defining the problem statement, the scope of the project, the data you have available, and the desired outcomes.\n",
    "\n",
    "2 - Gather and clean data: Data is the lifeblood of any machine learning model, so it's essential to gather as much relevant data as possible. This involves identifying the sources of data, cleaning the data to remove any errors or inconsistencies, and preparing the data in a format that's suitable for analysis.\n",
    "\n",
    "3 - Select the appropriate machine learning algorithm: There are many different machine learning algorithms available, each with its strengths and weaknesses. Choosing the right algorithm depends on the nature of the problem you're trying to solve and the type of data you're working with.\n",
    "\n",
    "4 - Train the model: Once you've selected the appropriate algorithm, you'll need to train the model using the data you've gathered. This involves feeding the data into the algorithm and adjusting the model's parameters until it produces accurate results.\n",
    "\n",
    "5 - Test and validate the model: After training the model, you'll need to test and validate it to ensure that it's working correctly. This involves running the model on a separate set of data that wasn't used during training and evaluating its performance.\n",
    "\n",
    "6 - Deploy the model: Once you're satisfied that the model is working correctly, you can deploy it in a production environment. This involves integrating the model into your application or system and ensuring that it's scalable and reliable.\n",
    "\n",
    "7 - Monitor and update the model: Machine learning models need to be monitored and updated regularly to ensure that they continue to produce accurate results. This involves analyzing the model's performance over time, identifying any issues or errors, and making updates or adjustments as necessary."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fe835c4",
   "metadata": {},
   "source": [
    "2.What are the different forms of data used in machine learning? Give a specific example for each of\n",
    "them.\n",
    "\n",
    "ANS-\n",
    "There are three main forms of data used in machine learning:\n",
    "\n",
    "1 - Structured data: Structured data refers to data that is organized into a specific format, such as a table or spreadsheet, with predefined columns and rows. This data is typically easy to analyze and process using traditional statistical methods. An example of structured data is a database of customer orders, with columns for order ID, customer ID, product name, price, and date.\n",
    "\n",
    "2 - Unstructured data: Unstructured data refers to data that is not organized in a specific format and does not have predefined columns or rows. This includes data like text, images, audio, and video. Unstructured data can be more challenging to analyze and process than structured data, but it can provide valuable insights when used in machine learning models. An example of unstructured data is a collection of customer reviews on a website, which may contain text, images, and ratings.\n",
    "\n",
    "3 - Semi-structured data: Semi-structured data is a combination of structured and unstructured data. It has some organizational structure, but the format may vary depending on the source of the data. Semi-structured data includes data like XML or JSON files, where the data is organized but does not have a specific schema. An example of semi-structured data is a collection of tweets, which may contain structured data like timestamps and usernames, as well as unstructured data like text and images.\n",
    "\n",
    "Here are some specific examples for each of them:\n",
    "\n",
    "1 - Structured data: A company has a database of sales transactions, with columns for date, customer ID, product ID, quantity, and price.\n",
    "2 - Unstructured data: A hospital has a collection of medical records, which includes handwritten notes from doctors, lab reports, and medical images.\n",
    "3 - Semi-structured data: An e-commerce website has a database of product reviews, which includes structured data like ratings and timestamps, as well as unstructured data like text and images."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d609fd2",
   "metadata": {},
   "source": [
    "3.Distinguish:\n",
    "\n",
    "1.Numeric vs. categorical attributes\n",
    "\n",
    "\n",
    "Numeric attributes are those that are represented by numerical values, such as age, weight, height, and income. These attributes can be continuous or discrete, and can be further categorized as either interval or ratio scales. Numeric attributes are typically used in regression and clustering models.\n",
    "\n",
    "Categorical attributes are those that are represented by non-numerical values, such as gender, color, brand, and product type. These attributes can be nominal or ordinal scales. Nominal attributes are those where the categories have no inherent order, such as gender or color, while ordinal attributes have categories with a specific order, such as low, medium, and high. Categorical attributes are typically used in classification models.\n",
    "\n",
    "2.Feature selection vs. dimensionality reduction\n",
    "\n",
    "\n",
    "Feature selection is the process of selecting the most relevant features or attributes from a dataset to use in a machine learning model. This involves identifying the most important features that contribute to the prediction of the target variable and removing any irrelevant or redundant features. Feature selection can be done using various techniques, such as correlation analysis, mutual information, and statistical tests.\n",
    "\n",
    "Dimensionality reduction is the process of reducing the number of features or variables in a dataset by transforming them into a lower-dimensional space. This is done to reduce the computational complexity of the model and improve its performance. Dimensionality reduction techniques include Principal Component Analysis (PCA), t-SNE, and LDA. Unlike feature selection, dimensionality reduction transforms the original features into a new set of features that may not be directly interpretable, but still retain the important information from the original data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5518e27b",
   "metadata": {},
   "source": [
    "4.Make quick notes on any two of the following:\n",
    "\n",
    "1.The histogram\n",
    "\n",
    "1 - A histogram is a graphical representation of the distribution of a dataset.\n",
    "2 - It displays the frequency or proportion of data points that fall within specific intervals or bins.\n",
    "3 - The x-axis represents the intervals, while the y-axis represents the frequency or proportion.\n",
    "4 - Histograms are useful for understanding the shape of a dataset, identifying outliers, and visualizing patterns in the data.\n",
    "\n",
    "2.Use a scatter plot\n",
    "\n",
    "1 - A scatter plot is a graphical representation of the relationship between two variables.\n",
    "2 - Each point on the plot represents a pair of values for the two variables being plotted.\n",
    "3 - Scatter plots can be used to identify patterns or trends in the data, and to detect outliers or anomalies.\n",
    "4 - Scatter plots are particularly useful for visualizing the relationship between two continuous variables, but can also be used for categorical variables with only a few unique values.\n",
    "\n",
    "3.PCA \n",
    "\n",
    "1 - PCA is a dimensionality reduction technique used to transform a high-dimensional dataset into a lower-dimensional space.\n",
    "2 - It works by identifying the principal components of the dataset, which are the directions of maximum variance.\n",
    "3 - The principal components are used to create a new set of variables that capture most of the variation in the original data.\n",
    "4 - PCA can be used to reduce the dimensionality of a dataset, to visualize high-dimensional data, and to identify the most important features or variables in a dataset.\n",
    "5 - PCA assumes that the data is linearly correlated and normally distributed, and may not work well for nonlinear or non-normal data.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dccea0c6",
   "metadata": {},
   "source": [
    "5.Why is it necessary to investigate data? Is there a discrepancy in how qualitative and quantitative\n",
    "data are explored?\n",
    "\n",
    "ANS-\n",
    "It is necessary to investigate data in order to gain a deeper understanding of the underlying patterns, relationships, and trends in the data. Data investigation involves examining the distribution, central tendency, variability, and relationships among variables. It can help to identify outliers, missing values, and data quality issues, which can have a significant impact on the accuracy and reliability of the analysis.\n",
    "\n",
    "Qualitative and quantitative data are explored differently because they have different characteristics and properties. Qualitative data is non-numeric and consists of descriptive information, such as words, images, and observations. Qualitative data is typically analyzed using methods such as content analysis, discourse analysis, or grounded theory. These methods involve identifying themes, patterns, and categories in the data, and interpreting the meaning and significance of these findings.\n",
    "\n",
    "Quantitative data, on the other hand, is numeric and consists of numerical measurements or counts. Quantitative data is typically analyzed using statistical methods such as descriptive statistics, correlation analysis, or regression analysis. These methods involve calculating summary statistics, exploring the distribution and variability of the data, and testing hypotheses about the relationships among variables."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cca8afe6",
   "metadata": {},
   "source": [
    "6.What are the various histogram shapes? What exactly are ‘bins'?\n",
    "\n",
    "ANS-\n",
    "1 - Normal distribution: A bell-shaped curve with a symmetrical distribution around the mean.\n",
    "2 - Skewed distribution: A distribution that is not symmetrical and has a tail on one side. It can be either positively skewed (tail on the right) or negatively skewed (tail on the left).\n",
    "3 - Bimodal distribution: A distribution with two peaks, indicating that the data may have two distinct subpopulations.\n",
    "4 - Uniform distribution: A distribution in which all values have approximately equal frequency.\n",
    "\n",
    "Bins in a histogram refer to the intervals into which the data is divided. The data is grouped into these intervals, and the number of data points falling within each interval is counted and represented on the y-axis of the histogram. The width of the bins can be chosen based on the range of the data, and the number of bins can be chosen based on the desired level of granularity in the analysis. Choosing the appropriate number of bins is important in order to accurately represent the distribution of the data, and can have an impact on the interpretation of the histogram."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f89e8b4",
   "metadata": {},
   "source": [
    "7.How do we deal with data outliers?\n",
    "\n",
    "ANS-\n",
    "1 - Identify the cause of the outliers: Before deciding how to deal with outliers, it is important to understand why they are occurring. Outliers may be due to measurement error, data entry errors, or a genuine anomaly in the data.\n",
    "\n",
    "2 - Remove the outliers: One approach to dealing with outliers is to remove them from the dataset. This can be done by using statistical techniques such as the interquartile range (IQR) or z-scores to identify extreme values, and then removing them from the dataset. However, this approach should be used with caution, as removing outliers can lead to loss of information and may introduce bias into the analysis.\n",
    "\n",
    "3 - Transform the data: Another approach to dealing with outliers is to transform the data using a mathematical function such as logarithmic or square root transformation. This can help to reduce the impact of extreme values on the analysis, but it may not be appropriate for all types of data.\n",
    "\n",
    "4 - Winsorize the data: Winsorizing is a technique that involves replacing extreme values with less extreme values. This can help to reduce the impact of outliers on the analysis while preserving the distribution of the data.\n",
    "Use robust statistical techniques: Robust statistical techniques, such as the median instead of the mean, are less sensitive to outliers and can be used to reduce the impact of extreme values on the analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e1eaaf1",
   "metadata": {},
   "source": [
    "8.What are the various central inclination measures? Why does mean vary too much from median in\n",
    "certain data sets?\n",
    "\n",
    "ANS-\n",
    "Central inclination measures are used to describe the central tendency or typical value of a dataset. The three most commonly used central inclination measures are the mean, median, and mode.\n",
    "\n",
    "The mean can vary too much from the median in certain data sets, especially when there are extreme values or outliers in the data. This is because the mean takes into account the value of each data point, whereas the median only considers the middle value. When extreme values are present in the data, they can greatly influence the value of the mean, but not the median. For example, if a dataset has a few very large values, the mean will be pulled towards these values, while the median will remain unaffected."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ded9d3c7",
   "metadata": {},
   "source": [
    "9.Describe how a scatter plot can be used to investigate bivariate relationships. Is it possible to find\n",
    "outliers using a scatter plot?\n",
    "\n",
    "ANS-\n",
    "A scatter plot is a graphical representation of the relationship between two variables in a dataset. It can be used to investigate bivariate relationships and to identify patterns or trends in the data. Each point on the scatter plot represents a pair of values for the two variables being analyzed.\n",
    "\n",
    "It is possible to identify outliers using a scatter plot. Outliers are data points that fall far away from the general pattern of the other data points. In a scatter plot, outliers are identified as points that fall far away from the general trend or pattern of the other points. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9548ddbc",
   "metadata": {},
   "source": [
    "10.Describe how cross-tabs can be used to figure out how two variables are related.\n",
    "\n",
    "ANS-\n",
    "Cross-tabulation, also known as contingency table or crosstab, is a statistical tool used to examine the relationship between two categorical variables. A cross-tabulation table displays the frequency distribution of two categorical variables in a matrix format, showing the number or proportion of observations in each combination of categories.\n",
    "\n",
    "To create a cross-tabulation table, the two categorical variables are placed on the rows and columns of the table. The frequency or proportion of each combination of categories is then calculated and displayed in the corresponding cell of the table."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
